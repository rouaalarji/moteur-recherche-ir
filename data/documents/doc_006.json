{
  "id": 6,
  "title": "TF-IDF",
  "url": "https://fr.wikipedia.org/wiki/Tf-idf",
  "date": "2025-11-29T22:26:37.633396",
  "content": "Pour les articles homonymes, voir fréquence (homonymie) .\nLe TF-IDF (de l'anglais term frequency-inverse document frequency ) est une méthode de pondération souvent utilisée en recherche d'information et en particulier dans la fouille de textes . Cette mesure statistique permet d'évaluer l'importance d'un terme contenu dans un document, relativement à une collection ou un corpus . Le poids augmente proportionnellement au nombre d'occurrences du mot dans le document. Il varie également en fonction de la répartition du mot dans le corpus. Des variantes de la formule originale sont souvent utilisées dans des moteurs de recherche pour apprécier la pertinence d'un document en fonction des critères de recherche de l'utilisateur.\nLa justification théorique a posteriori de ce schéma de pondération repose sur l'observation empirique de la fréquence des mots dans un texte qui est donnée par la loi de Zipf . Si une requête contient le terme T , un document a d'autant plus de chances d'y répondre qu'il contient ce terme : la fréquence du terme au sein du document (TF) est grande. Néanmoins, si le terme T est lui-même bien réparti au sein du corpus, c'est-à-dire qu'il est présent dans de nombreux documents (tels les articles définis le, la, les ), il est en fait peu discriminant. C'est pourquoi le schéma propose d'augmenter la pertinence d'un terme si sa répartition au sein du corpus est faible : l'inverse de la répartition du terme dans les documents du corpus (IDF) est grand. Ainsi, la présence d'un terme rare de la requête dans le contenu d'un document fait croître le « score » de ce dernier.\nLa fréquence « brute » d'un terme est simplement le nombre d'occurrences de ce terme dans le document considéré, divisé par le nombre de mots du document. On peut choisir cette fréquence brute pour exprimer la fréquence d'un terme.\nDes variantes ont été proposées. Un choix plus simple est le nombre d'occurrences de ce terme dans le document (appeler ce nombre « fréquence » est un abus de langage ). Un choix encore plus simple, dit « binaire », est de mettre 1 si le terme apparaît dans le document et 0 sinon. À l'opposé, on peut normaliser logarithmiquement la fréquence brute pour amortir les écarts. Une normalisation courante pour prendre en compte la longueur du document est de normaliser par la fréquence brute maximale du document.\nLa fréquence inverse de document ( inverse document frequency ) est une mesure de l'importance du terme dans l'ensemble du corpus. Dans le schéma TF-IDF, elle vise à donner un poids plus important aux termes les moins bien répartis, considérés comme plus discriminants. Elle consiste à calculer le logarithme (en base 10 ou en base 2 [ 1 ] ) de l'inverse de la proportion de documents du corpus qui contiennent le terme :\ni d f i = log ⁡ | D | | { d j : t i ∈ d j } | {\\displaystyle \\mathrm {idf_{i}} =\\log {\\frac {|D|}{|\\{d_{j}:t_{i}\\in d_{j}\\}|}}}\noù :\nFinalement, le poids s'obtient en multipliant les deux mesures :\nt f i d f i , j = t f i , j ⋅ i d f i {\\displaystyle \\mathrm {tfidf_{i,j}} =\\mathrm {tf_{i,j}} \\cdot \\mathrm {idf_{i}} }\nL'exemple porte sur le document 1 (soit d 1 {\\displaystyle d_{1}} ) et le terme analysé est « qui » (soit t 1 {\\displaystyle t_{1}} = qui). La ponctuation et l'apostrophe sont ignorées.\nTF(t) = Nombre d'apparitions du terme t dans le document / Nombre total de mots dans le document 1\nDétails du calcul : la plupart des termes apparaissent une fois (21 termes), arc , de , et , le , les , par et qui apparaissent 2 fois (7 termes) et l' apparaît 3 fois (1 terme). Le dénominateur est donc 21*1 + 7*2 + 1*3 = 38. Cette somme correspond au nombre de mots dans le document.\nLe terme qui n'apparaît pas dans le deuxième document et apparaît dans le premier et le troisième. Ainsi :\nOn obtient :\nPour les autres documents :\nLe premier document apparaît ainsi comme « le plus pertinent » pour le mot qui .\nEn recherche d'information , une fois un ensemble de documents potentiels identifiés comme pouvant répondre à une requête, il s'agit de les ordonner par ordre de pertinence. La pondération tf-idf est alors couramment utilisée pour établir la description des documents dans un modèle vectoriel , la similarité étant obtenue avec une distance cosinus entre le vecteur représentant la requête et chacun des vecteurs représentatifs des documents potentiels. Bien qu'établie dans les années 70, la variante Okapi BM25 est encore considérée (début XXI e siècle) comme l'une des méthodes à l'état de l'art dans ce domaine."
}