{
  "id": 3,
  "title": "Big data",
  "url": "https://fr.wikipedia.org/wiki/Big_data",
  "date": "2025-11-26T21:43:49.369523",
  "content": "Cet article possède un paronyme , voir Bigdatha .\nMégadonnées\nCet article sur l' informatique doit être recyclé ( novembre 2020 ).\nLe big data / ˌ b ɪ ɡ ˈ d e ɪ t ə / [ 1 ] ( litt. « grosses données » en anglais), les mégadonnées [ 2 ] , [ 3 ] ou les données massives [ 2 ] , désigne les ressources d’informations dont les caractéristiques en termes de volume, de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques particulières pour créer de la valeur [ 4 ] , [ 5 ] , et qui dépassent en général les capacités d'une seule et unique machine et nécessitent des traitements parallélisés.\nL’explosion quantitative (et souvent redondante ) des données numériques permet une nouvelle approche pour analyser le monde [ 6 ] . Le volume colossal de données numériques disponibles, implique de mettre en œuvre de nouveaux ordres de grandeur concernant la capture, le stockage, la recherche, le partage, l'analyse et la visualisation des données . Le traitement des big data [ 7 ] permet de nouvelles possibilités d'exploration de l'information et des données, celles-ci proviennent de nombreuses sources numériques : les réseaux sociaux, les médias [ 8 ] , l'OpenData, le Web, des bases de données privées, publiques à caractère commercial ou scientifique. Cela permet des recoupements et des analyses prédictives dans de nombreux domaines : scientifique, santé, économique, commercial… La multiplicité des applications a été comprise et développée par les plus gros acteurs du secteur des technologies de l'information [ 9 ] .\nDivers experts, grandes institutions (comme le MIT [ 10 ] aux États-Unis, le Collège de France [ 11 ] en Europe), administrations [ 12 ] et spécialistes sur le terrain des technologies ou des usages [ 13 ] considèrent le phénomène big data comme l'un des grands défis informatiques de la décennie 2010-2020 et en ont fait une de leurs nouvelles priorités de recherche et développement , qui pourrait notamment conduire à l' Intelligence artificielle en étant exploré par des réseaux de neurones artificiels autoapprenants [ 14 ] .\nLe big data a une histoire récente et pour partie cachée, en tant qu' espace virtuel prenant une importance volumique croissante dans le cyberespace et exploité, parfois illégalement, par des technologies de l'information ( moissonnage de données …).\nL'expression « big data » serait apparue en octobre 1997 selon les archives de la bibliothèque numérique de l' Association for Computing Machinery (ACM), dans un article scientifique sur les défis technologiques à relever pour visualiser les « grands ensembles de données » [ 18 ] .\nLa naissance du Big Data est liée aux progrès des capacités des systèmes de stockage, de fouille et d'analyse de l'information numérique, qui ont vécu une sorte de big bang des données [ 19 ] . Mais ses prémices sont à trouver dans le croisement de la cybernétique et de courants de pensée nés durant la Seconde Guerre mondiale , selon lesquels l’homme et le monde peuvent être représentés comme « des ensembles informationnels, dont la seule différence avec la machine est leur niveau de complexité. La vie deviendrait alors une suite de 0 et de 1 , programmable et prédictible » [ 20 ] .\nLes évolutions qui caractérisent le big data et ses algorithmes , ainsi que celles de la science des données sont en partie cachées (au sein des services de renseignement des grands États) et si rapides et potentiellement profondes que peu de prospectivistes se risquent à pronostiquer son devenir à moyen ou long terme [ 21 ] , mais la plupart des observateurs y voient des enjeux majeurs pour l'avenir, tant en termes d'opportunités commerciales [ 22 ] que de bouleversements sociopolitiques et militaires, avec en particulier le risque de voir émerger des systèmes ubiquistes , orwelliens [ 23 ] et totalitaires capables de fortement contrôler, surveiller et/ou influencer les individus et groupes.\nLes risques de dérives de la part de gouvernements ou d'entreprises [ 24 ] ont surtout d'abord été décrits par Orwell à la fin de la dernière guerre mondiale, puis souvent par la science fiction . Avec l'apparition de grandes banques de données dans les années 1970 (et durant toute la période de la guerre froide ) de nombreux auteurs s'inquiètent des risques pris concernant la protection de la vie privée [ 25 ] , en particulier Arthur R. Miller (en) qui cite l'exemple de la croissance des données stockées relatives à la santé physique et psychique des individus [ 26 ] , [ 27 ] , [ 28 ] .\nEn 2000, Froomkin, dans un article paru dans la revue Stanford Law Review , se demande si la vie privée n'est pas déjà morte [ 29 ] , mais ce sont surtout les révélations d' Edward Snowden (2013) qui ont suscité une nouvelle prise de conscience et d'importants mouvements de protestation citoyenne.\nLes quatre droits et « états de base de la vie privée » tels qu'énoncés par Westin en 1962 (droit à la solitude [ 30 ] , à l' intimité , à l' anonymat dans la foule et à la réserve ) sont menacés dans un nombre croissant de situations [ 31 ] , de même que la protection du contenu des courriers électroniques [ 32 ] qui fait partie du droit à la vie privée [ 33 ] .\nLe big data s'accompagne du développement d'applications à visée analytique, qui traitent les données pour en tirer du sens [ 34 ] . Ces analyses sont appelées big analytics [ 35 ] ou « broyage de données ». Elles portent sur des données quantitatives complexes à l'aide de méthodes de calcul distribué et de statistiques.\nEn 2001, un rapport de recherche du META Group (devenu Gartner ) [ 36 ] définit les enjeux inhérents à la croissance des données comme étant tri-dimensionnels : les analyses complexes répondent en effet à la règle dite « des 3V » (volume, vélocité et variété [ 37 ] ). Ce modèle est encore largement utilisé aujourd’hui pour décrire ce phénomène [ 38 ] . Aux 3 V initiaux, sont parfois ajoutés d'autres V comme : Véracité, Valeur et Visualisation [ 39 ] , [ 40 ] .\nC'est une dimension relative : le big data , comme le notait Lev Manovitch en 2011 [ 41 ] , définissait autrefois « les ensembles de données suffisamment grands pour nécessiter des super-ordinateurs » , mais il est rapidement (dans les années 1990/2000) devenu possible d'utiliser des logiciels standards sur des ordinateurs de bureau pour analyser ou co-analyser de vastes ensembles de données [ 42 ] .\nLe volume des données créées est en pleine expansion : les données numériques créées dans le monde seraient passées de 1,2 zettaoctet par an en 2010 à 1,8 zettaoctet en 2011 [ 43 ] , puis 2,8 zettaoctets en 2012 et s'élèveront à 64 zettaoctets en 2020 [ 44 ] , et 2 142 zettaoctets en 2035 [ 44 ] . À titre d'exemple, Twitter générait en janvier 2013, 7 téraoctets de données chaque jour et Facebook 10 téraoctets [ 45 ] . En 2014, Facebook Hive générait 4 000 To de data par jour [ 46 ] .\nLes installations technico-scientifiques (météorologie, etc. ) produiraient le plus de données [réf. nécessaire] . De nombreux projets de dimension pharaonique sont en cours. Le radiotélescope Square Kilometre Array par exemple produira 50 téraoctets de données analysées par jour, tirées de données brutes produites à un rythme de 7 000 téraoctets par seconde [ 47 ] .\nLe volume des big data met les centres de données face à un réel défi : la variété des données.\nIl ne s'agit pas uniquement de données relationnelles traditionnelles, mais surtout de données brutes, semi-structurées, voire non structurées (cependant, les données non structurées devront être analysées et structurées ultérieurement si nécessaire pour leur utilisation [ 48 ] ).\nCe sont des données complexes qui proviennent de multiples sources : du web ( Web mining ), de bases publiques (open data, Web des données ), géo-démographiques par îlot ( adresses IP ), machines ou objets connectés (IoT) , ou relever de la propriété des entreprises et des consommateurs [réf. nécessaire] , ce qui les rend inaccessibles aux outils traditionnels.\nLa démultiplication des outils de collecte sur les individus et sur les objets permet d’amasser toujours plus de données [ 49 ] . Les analyses sont d’autant plus complexes qu’elles portent de plus en plus sur les liens entre des données de natures différentes.\nLa vélocité représente la fréquence à laquelle les données sont à la fois engendrées, capturées, partagées et mises à jour [ 50 ] .\nDes flux croissants de données doivent être analysés en quasi-temps réel ( fouille de flots de données ) pour répondre aux besoins des processus chrono-sensibles [ 51 ] . Par exemple, les systèmes mis en place par la bourse et les entreprises doivent être capables de traiter ces données avant qu’un nouveau cycle de génération n’ait commencé, avec le risque pour l'Homme de perdre une grande partie de la maîtrise du système quand les principaux opérateurs deviennent des machines sans disposer de tous les critères pertinents d'analyse pour le moyen et long terme.\nLa véracité fait référence à la fiabilité et à la dimension qualitative des données. Traiter et gérer l’incertitude et les erreurs rencontrées dans certaines données, représente un challenge de taille pour fiabiliser et minimiser les biais [ 39 ] , [ 40 ] .\nLes efforts et les investissements dans l'utilisation et application Big Data n’ont de sens que si elles apportent de la valeur ajoutée [ 39 ] , [ 40 ] .\nLa mise en forme et mise à disposition des données et des résultats de l'analyse des données, permet de faciliter sa compréhension et son interprétation, afin d'améliorer la prise de décisions [ 39 ] .\nSi la définition du Gartner en 3V est encore largement reprise (voire augmentée de « V » supplémentaires selon l’inspiration des services marketing), la maturation du sujet fait apparaître un autre critère plus fondamental de différence avec l' informatique décisionnelle et concernant les données et leur utilisation [ 52 ] :\nSynthétiquement :\nLes bases de données relationnelles classiques ne permettent pas de gérer les volumes de données du big data. De nouveaux modèles de représentation permettent de garantir les performances sur les volumétries en jeu.\nCes technologies, dites de business analytics and optimization (BAO) permettent de gérer des bases massivement parallèles [ 56 ] .\nDes patrons d’architecture (« big data architecture framework », BDAF) [ 57 ] sont proposés par les acteurs de ce marché comme MapReduce créé par Google et utilisé dans le framework Hadoop . Avec ce système, les requêtes sont séparées et distribuées à des nœuds parallélisés, puis exécutées en parallèle (map). Les résultats sont ensuite rassemblés et récupérés (reduce). Teradata , Oracle ou EMC (via le rachat de Greenplum) proposent également de telles structures, basées sur des serveurs standards dont les configurations sont optimisées. Ils sont concurrencés par des éditeurs comme SAP et plus récemment Microsoft [ 58 ] .\nLes acteurs du marché s’appuient sur des systèmes à forte évolutivité horizontale et sur des solutions basées sur du NoSQL ( MongoDB , Cassandra ) plutôt que sur des bases de données relationnelles classiques [ 59 ] .\nPour répondre aux problématiques big data, l’architecture de stockage des systèmes doit être repensée et les modèles de stockage se multiplient en conséquence.\nLe big data trouve des applications dans de nombreux domaines : programmes scientifiques (CERN28 Mastodons), outils d'entreprises (IBM29, Amazon Web Services, BigQuery, SAP HANA) parfois spécialisées (Teradata, Jaspersoft30, Pentaho31…) ou startups, ainsi que dans le domaine de l'open source ( Apache Hadoop , Infobright32, Talend33…) et de logiciels d'exploitation ouverts (avec par exemple le logiciel ouvert d'analyse de big data H2O ).\nLes applications du Big data sont très nombreuses : il permet des recoupements et des analyses prédictives dans les domaines de connaissance et d'évaluation, d'analyse tendancielle et prospective (climatiques, environnementales ou encore sociopolitiques, etc.) et de gestion des risques (commerciaux, assuranciels, industriels, naturels) et de prise de décisions, et de phénomènes religieux, culturels, politiques [ 67 ] , mais aussi en termes de génomique ou métagénomique [ 68 ] , pour la médecine (compréhension du fonctionnement du cerveau , épidémiologie , écoépidémiologie …), la météorologie et l' adaptation aux changements climatiques , la gestion de réseaux énergétiques complexes (via les smartgrids ou un futur « internet de l'énergie »), l' écologie (fonctionnement et dysfonctionnement des réseaux écologiques, des réseaux trophiques avec le GBIF par exemple), ou encore la sécurité et la lutte contre la criminalité [ 69 ] , ou encore améliorer l'« expérience client » en la rendant plus personnalisée et contextualisée [ 70 ] . La multiplicité de ces applications laisse d'ailleurs déjà poindre un véritable écosystème économique impliquant, d'ores et déjà, les plus gros acteurs du secteur des technologies de l'information [ 9 ] .\nLe big data en est issu et il alimente une partie de la recherche. Ainsi le Large Hadron Collider du CERN utilise environ 150 millions de capteurs délivrant des données 40 millions de fois par seconde ; Pour 600 millions de collisions par seconde, il reste après filtrage 100 collisions d'intérêt par seconde, soit 25 Po de données à stocker par an, et 200 Po après réplication [ 71 ] , [ 72 ] , [ 73 ] . Les outils d'analyse du big data pourraient affiner l'exploitation de ces données.\nQuand le Sloan Digital Sky Survey (SDSS) a commencé à collecter des données astronomiques en 2000, il a amassé en quelques semaines plus de données que toutes celles précédemment collectées dans l’histoire de l’astronomie. Il continue à un rythme de 200 Go par nuit, et a en 10 ans (2000-2010) stocké plus de 140 téraoctets d’information. Le Large Synoptic Survey Telescope en amasse autant tous les sept jours, soit 20 téraoctet envoyés chaque nuit [ 74 ] .\nDécoder le premier génome humain a nécessité dix ans, mais prend aujourd'hui moins d'une semaine : les séquenceurs d'ADN ont progressé d'un facteur 10 000 les dix dernières années, soit 100 fois la loi de Moore (qui a progressé d'un facteur 100 environ sur 10 ans) [ 75 ] . En biologie, les approches massives basées sur une logique d’exploration des données et de recherche d’induction sont légitimes et complémentaires des approches classiques basées sur l'hypothèse initiale formulée [ 76 ] . Le big data s'est aussi introduit dans le domaine des protéines .\nLe NASA Center for Climate Simulation (NCCS) stocke 32 Po de données d’observations et de simulations climatiques [ 77 ] .\nLes sciences sociales explorent des corpus aussi variés que le contenu de Wikipédia dans le monde ou les millions de publications et de tweets sur Internet.\nLe big data mondial contient des données essentielles « pour résoudre l'équation climatique » , et notamment pour améliorer l' efficacité énergétique des villes et bâtiments, pour les smartgrids , pour vérifier l'application de règlementations visant à lutter contre la déforestation , la surpêche , la dégradation des sols , le gaspillage alimentaire ou à mieux gérer les déchets , éco-consommer ou inciter les investisseurs à créer des villes intelligentes [ 78 ] , etc.\nLors de la COP 23 (Bonn, 2017) un événement parallèle de haut niveau organisé par le « Forum sur l'innovation durable » et le PNUD a réuni des dirigeants de sociétés de données du secteur privé et des représentants des Nations unies. Ce groupe a appelé à développer la « philanthropie des données » , c'est-à-dire à massivement et de manière altruiste partager les données [ 79 ] , [ 80 ] pour stimuler l'efficacité, l'innovation et le soutien aux actions de protection du climat et de résilience face au changement climatique. Une meilleure collecte, mise à disposition de tous, analyse et utilisation des données volumineuses est une condition selon ce groupe pour atteindre l'objectif 2030 n o 13 (pour le climat) de l'ONU [ 81 ] et les objectifs de l' Accord de Paris sur le climat [ 78 ] .\nC'est ce qu'y a rappelé Amina J. Mohammed, Secrétaire générale adjointe des Nations unies, dans son discours d'ouverture. C'est le cas notamment des données météo nécessaires à l'agriculture, à la protection de l'économie et des infrastructures vulnérables aux aléas climatiques [ 78 ] .\nEn 2017, le PNUD aide plus de 75 pays à moderniser leurs systèmes de surveillance météorologique et climatiques. Dans les pays dits émergents, un effort reste à faire pour le « dernier kilomètre » ; par exemple, les « opérateurs mobiles » pourraient mieux recevoir l'information météorologique et aider à un partage des données sur les récoltes et problèmes de culture via des téléphones portables ; les antennes relais pourraient elles-mêmes, en lien avec des sociétés de Big Data devenir des plates-formes de regroupement de données utiles à l'élaboration de plans locaux et nationaux d'adaptation au changement climatique, et utiles à l'élaboration de stratégies sectorielles de résilience climatique [ 78 ] .\nLes difficultés d' anonymisation de la donnée privée restent cependant un important frein au partage efficace de données massives entre les décideurs et le grand public . La « philanthropie des données » vise à faire des secteurs public et privé deux partenaires égaux [ 78 ] .\nEn 2016, Linnet Taylor s'interroge : Quand le big data est présenté comme un commun ou un bien public ; de quel bien parle-t-on ? et à quel public le destine-t-on réellement [ 82 ] ? en citant notamment Robert Kirkpatrick (directeur de UN Global Pulse) pour qui « le big data est comme un nouveau type de ressource naturelle (ou non-naturelle) infiniment renouvelable, de plus en plus omniprésente - mais qui est tombée entre les mains d'une industrie extractive opaque et largement non réglementée, qui commence seulement à se rendre compte qu'il existe une opportunité sociale - et peut-être une responsabilité sociale - à s'assurer que ces données atteignent les personnes qui en ont le plus besoin » [ 79 ] , [ 82 ] .\nL’analyse du big data a joué un rôle important dans la campagne de réélection de Barack Obama , notamment pour analyser les opinions politiques de la population [ 83 ] , [ 84 ] , [ 85 ] .\nDepuis 2012, le département de la Défense américain investit annuellement sur les projets big data plus de 250 millions de dollars [ 86 ] . Le gouvernement américain possède six des dix plus puissants supercalculateurs de la planète [ 87 ] . La National Security Agency a notamment construit le Utah Data Center qui stocke depuis septembre 2014 jusqu'à un yottaoctet d’informations collectées par la NSA sur internet [ 88 ] . En 2013, le big data faisait partie des sept ambitions stratégiques de la France déterminées par la Commission innovation 2030 [ 89 ] .\nLa revente de fichier de profil utilisateur peut participer au big data.\nWalmart traite plus d'un million de transactions client par heure, importées dans des bases de données qui contiendraient plus de 2,5 Po d’information [ 90 ] . Facebook traite 50 milliards de photos. D’une manière générale l' exploration de données de big data permet l’élaboration de profils clients dont on ne supposait pas l’existence [ 91 ] .\nLe musée Solomon R. Guggenheim construit sa stratégie en analysant des données massives : dans les salles des transmetteurs électroniques suivent les visiteurs tout au long de leur visite. Le musée détermine ainsi de nouveaux parcours de visite en fonction des œuvres les plus appréciées, ou décider des expositions à mettre en place [ 92 ] .\nLes bâtiments intelligents (éventuellement au sein de villes intelligentes ) sont caractérisés par une « hybridation » entre numérique et énergie .\nCes bâtiments ou logements individuels peuvent produire de l'énergie (voire être positifs en énergie ). Ils peuvent aussi produire des données sur cette énergie et/ou sur leur consommation d'énergies. Ces données une fois agrégées et analysées peuvent permettre d'appréhender voire d'anticiper la consommation des usagers, des quartiers, villes, etc. en fonction des variations du contexte, météorologique notamment.\nL'analyse des données collectées de production (solaire, microéolien…) et de consommation dans un bâtiment, par le biais des objets connectés et du smartgrid , permet aussi potentiellement de mieux gérer la consommation des usagers (de manière personnalisée).\nEn attendant un développement plus large du stockage de l'énergie , les jours nuageux et sans vent il faut encore faire appel à des centrales conventionnelles, et les jours exceptionnellement beaux et venteux (ex. : en Allemagne, 8 mai 2016 où durant 4 heures le vent et le soleil ont engendré plus de 90 % de l'électricité du pays [réf. nécessaire] , les centrales électriques au charbon et au gaz doivent réduire à temps leur production). Un cas extrême est celui d’une éclipse solaire (prévisible). La gestion de ces pics et intermittences coûte aujourd’hui plus de 500 millions €/an à l’Allemagne et conduit à des émissions de CO 2 et autres gaz à effet de serre que l’on voudrait éviter [ 93 ] . Grâce aux corrélations pouvant émerger de l'analyse fine des mégadonnées, les opérateurs de l'énergie peuvent mieux appréhender les variations fines du gisement des énergies renouvelables et les croiser avec la demande réelle.\nExemples\nDans la majorité des cas, les entreprises peuvent utiliser les données pour mieux connaitre leur marché. En effet les données collectées par les cartes de fidélité et les historiques d’achat permettent de mieux comprendre le marché de manière générale, d’en faire une meilleure segmentation [ 95 ] . Les entreprises vont pouvoir proposer des articles qui correspondent aux envies du clients par le ciblage. Le meilleur exemple serait Amazon qui, grâce au big data, a réussi à accroitre la pertinence de ses recommandations [ 96 ] . Le Big Data permet donc de dégager un schéma global aidant à comprendre le marché. L’entreprise saura alors quels produits proposés ou sur quels produits il faut davantage accentuer la communication afin de les rendre plus attrayants [ 97 ] . Tout cela peut être crucial pour l’entreprise. Mais elles peuvent aussi utiliser les données dans un autre registre : améliorer leurs technologies. Par exemple Rolls-Royce met des capteurs dans les moteurs de leurs réacteurs afin de d’avoir de multiples informations pendant le vol [ 96 ] . Cet auteur explique qu’avant le boom du big data, les informations jugées superflues étaient détruites par les ordinateurs mais maintenant elles sont collectées dans des serveurs centraux afin de créer des modèles permettant de prévoir des pannes et/ou des défaillances. Elle a donc renforcé la sureté de ses réacteurs et a pu transformer ces données en profit.\nL'un des principaux enjeux de productivité du big data dans son évolution va porter sur la logistique de l'information, c'est-à-dire sur la manière de garantir que l'information pertinente arrive au bon endroit au bon moment. Il s'agit d'une approche micro-économique. Son efficacité dépendra ainsi de celle de la combinaison entre les approches micro- et macro-économique d'un problème.\nSelon certaines sources, les données numériques créées dans le monde atteindraient 47 zettaoctets d'ici 2020 [ 44 ] et 175 zettaoctets en 2035 [ 44 ] . À titre de comparaison, Facebook générait environ 10 téraoctets de données par jour au début 2013. Le développement de l'hébergement massif de données semble avoir été accéléré par plusieurs phénomènes simultanément : la pénurie de disques durs à la suite des inondations en Thaïlande en 2011, l'explosion du marché des supports mobiles (smartphones et tablettes notamment), etc. Ajouté à cela, la démocratisation du cloud-computing de plus en plus proche, grâce à des outils comme Dropbox, amène le big data au centre de la logistique de l'information.\nAfin de pouvoir exploiter au maximum le big data , de nombreuses avancées doivent être faites, et ce en suivant trois axes.\nLes méthodes de modélisation de données ainsi que les systèmes de gestion de base de données relationnelles classiques ont été conçus pour des\nvolumes de données très inférieurs. La fouille de données a des caractéristiques fondamentalement différentes et les technologies actuelles ne permettent pas de les exploiter.\nDans le futur il faudra des modélisations de données et des langages de requêtes permettant :\nDe très nombreux autres thèmes de recherche sont liés à ce thème, citons notamment : la réduction de modèle pour les EDP, l' acquisition comprimée en imagerie, l'étude de méthodes numériques d'ordre élevé… Probabilités, statistiques, analyse numérique, équations aux dérivées partielles déterministes et stochastiques, approximation, calcul haute performance, algorithmique… Une grande partie de la communauté scientifique, notamment en mathématiques appliquées et en informatique, est concernée par ce thème porteur.\nLe besoin de gérer des données extrêmement volumineuses est flagrant et les technologies d’aujourd’hui [Quand ?] [réf. nécessaire] ne permettent pas de le faire. Il faut repenser des concepts de base de la gestion de données qui ont été déterminés dans le passé. Pour la recherche scientifique, par exemple, il sera indispensable de reconsidérer le principe qui veut qu’une requête sur un SGBD fournisse une réponse complète et correcte sans tenir compte du temps ou des ressources nécessaires. En effet la dimension exploratoire de la fouille de données fait que les scientifiques ne savent pas nécessairement ce qu’ils cherchent. Il serait judicieux que le SGBD puisse donner des réponses rapides et peu coûteuses qui ne seraient qu’une approximation, mais qui permettraient de guider le scientifique dans sa recherche [ 98 ] .\nDans le domaine des données clients, il existe également de réels besoins d'exploitation de ces données, en raison notamment de la forte augmentation de leur volume des dernières années [ 100 ] . Le big data et les technologies associées permettent de répondre à différents enjeux tels que l'accélération des temps d’analyse des données clients, la capacité à analyser l’ensemble des données clients et non seulement un échantillon de celles-ci ou la récupération et la centralisation de nouvelles sources de données clients à analyser afin d’identifier des sources de valeur pour l’entreprise.\nLes outils utilisés au debut des années 2010 ne sont pas en adéquation avec les volumes de données engendrés dans l’exploration du big data . Il est nécessaire de concevoir des instruments permettant de mieux visualiser , analyser, et cataloguer les ensembles de données afin de permettre une optique de recherche guidée par la donnée [ 98 ] . La recherche en big data ne fait que commencer. La quantité de données évolue beaucoup plus rapidement que nos connaissances sur ce domaine. Le site The Gov Lab prévoit qu'il n y aura pas suffisamment de scientifiques du data . En 2018, les États-Unis auraient besoin de 140 000 à 190 000 scientifiques spécialisés en big data [ 86 ] .\nLe déluge de données qui alimente le big data (et dont certaines sont illégales ou incontrôlées) est souvent métaphoriquement comparé à la fois à un flux continu de nourriture, de pétrole ou d’énergie (qui alimente les entreprises du data mining et secondairement la société de l’information [ 101 ] ) qui expose au risque d’ infobésité et pourrait être comparé à l’équivalent d’une « pollution » [ 42 ] du cyberespace et de la noosphère (métaphoriquement, le big data correspondrait pour partie à une sorte de grande marée noire informationnelle, ou à une eutrophisation diffuse mais croissante et continue du monde numérique pouvant conduire à une dystrophisation, voire à des dysfonctions au sein des écosystèmes numériques) [ 102 ] .\nFace à cette « entropie informationnelle » quelques réponses de type néguentropique sont nées ( Wikipédia en fait partie en triant et restructurant de l’information déjà publiée).\nD’autres réponses ont été la création de moteurs de recherche et d’outils d’analyse sémantique et de fouille de flots de données , de plus en plus puissants et rapides.\nNéanmoins, l'analyse du big data tend elle-même à engendrer du big data, avec un besoin de stockage et de serveurs qui semble exponentiel.\nParallèlement à la croissance de la masse et du flux de données, une énergie croissante est dépensée d'une part dans la course aux outils de datamining, au chiffrement/déchiffrement et aux outils analytiques et d’authentification, et d'autre part dans la construction de fermes de serveurs qui doivent être refroidis ; au détriment du bilan énergétique et électrique du Web.\nEn 2010, les jeux de données produites par l’homme sont de plus en plus complétés par d'autres données, massivement acquises de manière passive et automatique par un nombre croissant de capteurs électroniques et sous des formes de plus en plus interopérables et compréhensibles par les ordinateurs. Le volume de données crées dans le monde fait plus que doubler tous les deux ans, et en migrant de plus en plus sur internet, les uns voient dans le big data intelligemment utilisé une source d’information qui permettrait de lutter contre la pauvreté, la criminalité ou la pollution. Et à l'autre extrémité du spectre des avis, d'autres, souvent défenseurs de la confidentialité de la vie privée, en ont une vision plus sombre, craignant ou affirmant que le big data est plutôt un Big Brother se présentant dans de « nouveaux habits » [ 103 ] , « dans des vêtements de l’entreprise » [ 104 ] .\nEn 2011 à l'occasion d'un bilan sur 10 ans d'Internet pour la société, Danah Boyd (de Microsoft Research ) et Kate Crawford ( University of New South Wales ) dénonçaient de manière provocatrice six problèmes liés à des idées reçues sur le big data [ 105 ] : « L’automatisation de la recherche change la définition du savoir (…) Les revendications d’objectivité et d’exactitude sont trompeuses (…) De plus grosses données ne sont pas toujours de meilleures données (…) Toutes les données ne sont pas équivalentes (…) Accessible ne signifie pas éthique (…) L’accès limité aux big data crée de nouvelles fractures numériques » [ 42 ] entre les chercheurs ayant accès aux données de l'intérieur ou en payant ce droit d'accès [ 42 ] .\nLes études critiques sur les données relèvent un ensemble de problématiques liées aux données massives. Plusieurs types de risques d'atteinte à la vie privée et aux droits fondamentaux sont cités par la littérature :\nLa Commissaire européenne à la Concurrence, Margrethe Vestager , a considéré auprès du Wall Street Journal que les grandes sociétés pouvaient utiliser des masses gigantesques de données d’utilisateurs pour entraver la concurrence [ 131 ] .\nDans un rapport du CIB (Comité International de Bioéthique) sur les mégadonnées et la santé, publié en 2015, il mentionne que « L’enthousiasme suscité par le phénomène des mégadonnées risque d’entraîner des suréstimations et des prévisions irréalistes » [ 132 ] . Cela peut « mener à un déséquilibre des priorités en termes de politiques de santé, notamment dans les pays où l'accès à ces services essentiels n'est pas garanti » . En conclusion de la proposition 45, le CIB précise qu' « Il est par conséquent essentiel de gérer avec bon sens l’optimisme suscité par ce phénomène » .\nLa gouvernance des données peut se faire au niveau des entreprises, dans l'objectif de gérer efficacement leurs données; et aussi des états, pour réguler le bon usage des données.\nElle nécessite un débat citoyen constant [ 133 ] ainsi que des modes de gouvernance et de surveillance adaptés [ 134 ] car des États, des groupes ou des entreprises ayant des accès privilégiés au big data peuvent en extraire très rapidement un grand nombre de « données personnelles diffuses » qui, par croisement et analyse, permettent un profilage de plus en plus précis, intrusif et parfois illégal (faisant fi de la protection de la vie privée ) des individus, des groupes, des entreprises, et en particulier de leur statut social, culturel, religieux ou professionnel (exemple du programme PRISM de la NSA ), de leurs activités personnelles, leurs habitudes de déplacement, d’achat et de consommation, ou encore de leur santé. Cette question renvoie directement à la Déclaration Universelle des droits de l'Homme qui indique, dans l'article 12, que « Nul ne sera l'objet d'immixtions arbitraires dans sa vie privée, sa famille, son domicile ou sa correspondance, ni d'atteintes à son honneur et à sa réputation. Toute personne a droit à la protection de la loi contre de telles immixtions ou de telles atteintes » [ 135 ] . « La montée des big data amène aussi de grandes responsabilités » [ 42 ] . En matière de santé publique notamment, des enjeux éthiques forts existent [ 136 ] .\nSur la scène européenne, un nouveau règlement a été mis en place dans le courant de l'année 2015 : le RGPD ou GDPR (General Data Protection Regulation). Il s'agit d'un règlement qui modifie le cadre juridique relatif à la protection des données personnelles au sein de l’union européenne. Le RGPD rappelle que toute personne physique devrait avoir le contrôle de données à caractère personnel la concernant. Toute opération économique se doit, de plus, d'être transparente, le règlement en assure la sécurité juridique (article 13). Enfin la protection des données personnelles est garantie par ce nouveau règlement (article 17) [ 137 ] .\nLes plateformes big data sont conçues pour traiter une quantité de données massive, en revanche elles sont très rarement conçues pour traiter ces données en temps réel. Les nouveaux usages et les nouvelles technologies engendrent des données au quotidien et sans interruption, il est donc nécessaire de faire évoluer ces plateformes pour traiter les données temps réel afin de répondre aux exigences métiers qui demandent d’aller vers plus de réactivité et de personnalisation. C’est la raison pour laquelle les architectures lambda et kappa ont vu le jour. Ces architectures permettent de prendre en compte les flux de données temps réel pour répondre à ces nouvelles exigences [ 138 ] .\nSur les autres projets Wikimedia :"
}